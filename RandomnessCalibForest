import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import check_array, check_X_y, check_random_state
from sklearn.utils.validation import check_is_fitted

class RandomnessCalibForest(BaseEstimator, ClassifierMixin):
    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,
                 min_samples_leaf=1, max_features='auto', random_state=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.random_state = random_state

    def fit(self, X, y):
        # Check that X and y have correct shape
        X, y = check_X_y(X, y)
        
        # Store the classes seen during fit
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        
        # Check random state
        self.random_state_ = check_random_state(self.random_state)
        
        # Estimate data randomness
        self.data_randomness_ = self._estimate_data_randomness(X)
        
        # Initialize list to store estimators
        self.estimators_ = []
        
        for _ in range(self.n_estimators):
            tree = DecisionTreeClassifier(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf,
                max_features=self._calibrate_max_features(self.data_randomness_, X.shape[1]),
                random_state=self.random_state_.randint(np.iinfo(np.int32).max)
            )
            
            # Calibrate bootstrap sample size
            sample_size = self._calibrate_sample_size(self.data_randomness_, X.shape[0])
            bootstrap_indices = self.random_state_.choice(X.shape[0], size=sample_size, replace=True)
            
            X_bootstrap = X[bootstrap_indices]
            y_bootstrap = y[bootstrap_indices]
            
            tree.fit(X_bootstrap, y_bootstrap)
            self.estimators_.append(tree)
        
        return self

    def predict(self, X):
        # Check is fit had been called
        check_is_fitted(self, ['estimators_', 'classes_'])
        
        # Input validation
        X = check_array(X)
        
        # Predict
        predictions = np.array([tree.predict(X) for tree in self.estimators_])
        maj = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions)
        
        return self.classes_[maj]

    def _estimate_data_randomness(self, X):
        # Improved method to estimate data randomness
        feature_variances = np.var(X, axis=0)
        feature_ranges = np.ptp(X, axis=0)
        normalized_variances = feature_variances / (feature_ranges ** 2 + 1e-10)
        return np.mean(normalized_variances)

    def _calibrate_max_features(self, data_randomness, n_features):
        if self.max_features == 'auto':
            base_features = int(np.sqrt(n_features))
        elif isinstance(self.max_features, int):
            base_features = min(self.max_features, n_features)
        elif isinstance(self.max_features, float):
            base_features = int(min(self.max_features * n_features, n_features))
        else:
            raise ValueError("Invalid value for max_features")
        
        # Adjust max_features based on data randomness
        adjustment_factor = 1 + (1 - data_randomness)
        return max(1, min(int(base_features * adjustment_factor), n_features))

    def _calibrate_sample_size(self, data_randomness, n_samples):
        base_size = int(0.632 * n_samples)  # Default bootstrap sample size
        
        # Adjust sample size based on data randomness
        adjustment_factor = 1 + data_randomness
        return min(int(base_size * adjustment_factor), n_samples)